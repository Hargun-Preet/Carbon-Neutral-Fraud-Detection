# AI-Based Carbon-Neutral Fraud Detection

![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Kaggle](https://img.shields.io/badge/dataset-IEEE--CIS-blue)
![Codecarbon](https://img.shields.io/badge/tracked%20by-CodeCarbon-green)

A Python project that compares the performance, energy consumption, and carbon footprint of a Deep Neural Network (DNN) vs. a LightGBM model for financial fraud detection.

## 1. Project Hypothesis

The AI industry is facing a critical challenge: state-of-the-art models, particularly Deep Neural Networks, consume massive amounts of energy, contributing to a significant carbon footprint. This project investigates this trade-off in the critical domain of financial fraud detection.

**Hypothesis:** An energy-efficient gradient boosting model (LightGBM) can achieve **superior** fraud detection performance on large-scale tabular data while using **dramatically less energy** and producing fewer CO₂ emissions than a high-energy Deep Neural Network (DNN) baseline.

## 2. Methodology

This project implements a three-phase evaluation framework:

### Phase 1: Dataset

- **Source:** [IEEE-CIS Fraud Detection Dataset (Kaggle)](https://www.kaggle.com/c/ieee-fraud-detection/data)
- **Details:** A large-scale, real-world dataset containing transaction and identity data, known for its high-dimensionality and severe class imbalance.

### Phase 2: Modeling

- **Baseline (High-Energy):** A multi-layer Deep Neural Network (DNN) built with `TensorFlow/Keras`, complete with Batch Normalization and Dropout layers.
- **Proposed (Energy-Efficient):** A LightGBM (Gradient Boosting Machine) model, a tree-based algorithm renowned for its high performance and efficiency on tabular data.

### Phase 3: Evaluation Framework

- **Performance Metrics:** F1-Score, AUC, Precision, and Recall.
- **Sustainability Metrics:** Training Time (s), Inference Latency (ms), Total Energy Consumed (Wh), and Total CO₂ Emissions (g).

All sustainability metrics are tracked in real-time using the `codecarbon` library, which measures power draw from the CPU and GPU.

## 3. Results & Conclusion

The experiment was executed on a machine with a 13th Gen Intel i7 CPU and an NVIDIA RTX 4050 Laptop GPU. The results conclusively support the hypothesis.

### Final Comparison Table

| Model                   |  F1-Score  |    AUC     | Precision  |   Recall   | Training Time (s) | Inference Latency (ms/item) | Energy (Wh) | CO₂ Emissions (g) |
| :---------------------- | :--------: | :--------: | :--------: | :--------: | :---------------: | :-------------------------: | :---------: | :---------------: |
| **DNN (Baseline)**      |   0.4639   |   0.8885   |   0.9133   |   0.3109   |       56.32       |           0.0082            |   0.8816    |      0.0035       |
| **LightGBM (Proposed)** | **0.7970** | **0.9724** | **0.9551** | **0.6838** |     **32.84**     |           0.0117            | **0.5543**  |    **0.0022**     |

### Key Findings

1.  **Performance:** The energy-efficient **LightGBM model was not just comparable—it was dramatically superior** in every key performance metric. Most critically, it **found 68.4% of all fraud (Recall)**, while the high-energy DNN missed almost 70% of it.

2.  **Sustainability:** The LightGBM model was **quantifiably "greener."** It trained **41.7% faster** than the DNN while consuming **37.2% less energy** and producing **37.1% less carbon**.

### Conclusion

This analysis proves that for this large-scale, tabular data problem, **LightGBM is not a compromise; it is the superior choice in every category.** It delivers a more accurate, reliable, and faster model while being significantly more energy-efficient and environmentally friendly than the DNN baseline.

## 4. How to Run This Project

You can replicate this experiment by following these steps:

### Step 1: Download the Dataset

This script **requires** the IEEE-CIS dataset from Kaggle.

1.  Go to: <https://www.kaggle.com/c/ieee-fraud-detection/data>
2.  Download the following two files:
    - `train_transaction.csv`
    - `train_identity.csv`
3.  Place both `.csv` files in the **same directory** as the `run_analysis.py` script.

### Step 2: Set Up Your Environment

It is highly recommended to use a Python virtual environment.

````bash
# Create a virtual environment
python -m venv venv

# Activate it (Windows)
.\venv\Scripts\activate
# (macOS/Linux)
# source venv/bin/activate
You are completely right! My apologies. The chat interface is rendering the Markdown.

Here is the plain, raw text enclosed in a code block so you can copy-paste it without any formatting.

Markdown

# AI-Based Carbon-Neutral Fraud Detection

![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Kaggle](https://img.shields.io/badge/dataset-IEEE--CIS-blue)
![Codecarbon](https://img.shields.io/badge/tracked%20by-CodeCarbon-green)

A Python project that compares the performance, energy consumption, and carbon footprint of a Deep Neural Network (DNN) vs. a LightGBM model for financial fraud detection.

## 1. Project Hypothesis

The AI industry is facing a critical challenge: state-of-the-art models, particularly Deep Neural Networks, consume massive amounts of energy, contributing to a significant carbon footprint. This project investigates this trade-off in the critical domain of financial fraud detection.

**Hypothesis:** An energy-efficient gradient boosting model (LightGBM) can achieve **superior** fraud detection performance on large-scale tabular data while using **dramatically less energy** and producing fewer CO₂ emissions than a high-energy Deep Neural Network (DNN) baseline.

## 2. Methodology

This project implements a three-phase evaluation framework:

### Phase 1: Dataset

* **Source:** [IEEE-CIS Fraud Detection Dataset (Kaggle)](https://www.kaggle.com/c/ieee-fraud-detection/data)
* **Details:** A large-scale, real-world dataset containing transaction and identity data, known for its high-dimensionality and severe class imbalance.

### Phase 2: Modeling

* **Baseline (High-Energy):** A multi-layer Deep Neural Network (DNN) built with `TensorFlow/Keras`, complete with Batch Normalization and Dropout layers.
* **Proposed (Energy-Efficient):** A LightGBM (Gradient Boosting Machine) model, a tree-based algorithm renowned for its high performance and efficiency on tabular data.

### Phase 3: Evaluation Framework

* **Performance Metrics:** F1-Score, AUC, Precision, and Recall.
* **Sustainability Metrics:** Training Time (s), Inference Latency (ms), Total Energy Consumed (Wh), and Total CO₂ Emissions (g).

All sustainability metrics are tracked in real-time using the `codecarbon` library, which measures power draw from the CPU and GPU.

## 3. Results & Conclusion

The experiment was executed on a machine with a 13th Gen Intel i7 CPU and an NVIDIA RTX 4050 Laptop GPU. The results conclusively support the hypothesis.

### Final Comparison Table

| Model | F1-Score | AUC | Precision | Recall | Training Time (s) | Inference Latency (ms/item) | Energy (Wh) | CO₂ Emissions (g) |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **DNN (Baseline)** | 0.4639 | 0.8885 | 0.9133 | 0.3109 | 56.32 | 0.0082 | 0.8816 | 0.0035 |
| **LightGBM (Proposed)** | **0.7970** | **0.9724** | **0.9551** | **0.6838** | **32.84** | 0.0117 | **0.5543** | **0.0022** |

### Key Findings

1.  **Performance:** The energy-efficient **LightGBM model was not just comparable—it was dramatically superior** in every key performance metric. Most critically, it **found 68.4% of all fraud (Recall)**, while the high-energy DNN missed almost 70% of it.

2.  **Sustainability:** The LightGBM model was **quantifiably "greener."** It trained **41.7% faster** than the DNN while consuming **37.2% less energy** and producing **37.1% less carbon**.

### Conclusion

This analysis proves that for this large-scale, tabular data problem, **LightGBM is not a compromise; it is the superior choice in every category.** It delivers a more accurate, reliable, and faster model while being significantly more energy-efficient and environmentally friendly than the DNN baseline.

## 4. How to Run This Project

You can replicate this experiment by following these steps:

### Step 1: Download the Dataset

This script **requires** the IEEE-CIS dataset from Kaggle.

1.  Go to: <https://www.kaggle.com/c/ieee-fraud-detection/data>
2.  Download the following two files:
    * `train_transaction.csv`
    * `train_identity.csv`
3.  Place both `.csv` files in the **same directory** as the `run_analysis.py` script.

### Step 2: Set Up Your Environment

It is highly recommended to use a Python virtual environment.

```bash
# Create a virtual environment
python -m venv venv

# Activate it (Windows)
.\venv\Scripts\activate
# (macOS/Linux)
# source venv/bin/activate
````

### Step 3: Install Requirements

Install all necessary libraries:

```bash
pip install pandas numpy scikit-learn tensorflow lightgbm codecarbon
```

### Step 4: Run the Analysis

Execute the script from your terminal:

```bash
python run_analysis.py
```

The script will load the data, run the full preprocessing pipeline, train both models, and print the final comparison table to your console. It will also generate an emissions.csv file in the same directory with a detailed log from codecarbon.
